{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForTokenClassification, AdamW\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import BertTokenizerFast\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "LANGUAGE_CODE = 'fr'\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-multilingual-cased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    tokens, labels = [], []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        current_tokens, current_labels = [], []\n",
    "        for line in file:\n",
    "            if line.strip() == \"\":\n",
    "                if current_tokens:\n",
    "                    tokens.append(current_tokens)\n",
    "                    labels.append(current_labels)\n",
    "                    current_tokens, current_labels = [], []\n",
    "            else:\n",
    "                parts = line.strip().split()\n",
    "                current_tokens.append(parts[0])\n",
    "                current_labels.append(parts[-1])\n",
    "        if current_tokens:\n",
    "            tokens.append(current_tokens)\n",
    "            labels.append(current_labels)\n",
    "    return tokens, labels\n",
    "\n",
    "def encode_tags(tags, encodings):\n",
    "    labels = [[tag2id[tag] for tag in doc] for doc in tags]\n",
    "    encoded_labels = []\n",
    "\n",
    "    for doc_labels, doc_offset in zip(labels, encodings.offset_mapping):\n",
    "\n",
    "        doc_enc_labels = np.ones(len(doc_offset), dtype=int) * -100\n",
    "\n",
    "        label_position = 0\n",
    "        for i, (start, end) in enumerate(doc_offset):\n",
    "            if start != 0:\n",
    "                continue\n",
    "\n",
    "            if label_position >= len(doc_labels):\n",
    "                continue\n",
    "            doc_enc_labels[i] = doc_labels[label_position]\n",
    "            \n",
    "        encoded_labels.append(doc_enc_labels.tolist())\n",
    "\n",
    "    return encoded_labels\n",
    "\n",
    "def create_dataset(token_lists, tag_lists):\n",
    "    encodings = tokenizer(token_lists, is_split_into_words=True, return_offsets_mapping=True, padding=True, truncation=True, max_length=MAX_LEN)\n",
    "    tags = encode_tags(tag_lists, encodings)\n",
    "    input_ids = torch.tensor(encodings['input_ids'])\n",
    "    attention_masks = torch.tensor(encodings['attention_mask'])\n",
    "    labels = torch.tensor(tags)\n",
    "    return TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Data and Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "train_path = f'E:\\\\MultiCoNER_2_train_dev\\\\train_dev\\\\{LANGUAGE_CODE}-train.conll'\n",
    "dev_path = f'E:\\\\MultiCoNER_2_train_dev\\\\train_dev\\\\{LANGUAGE_CODE}-dev.conll'\n",
    "\n",
    "train_tokens, train_tags = load_data(train_path)\n",
    "dev_tokens, dev_tags = load_data(dev_path)\n",
    "\n",
    "tag2id = {tag: idx for idx, tag in enumerate(np.unique([tag for sublist in train_tags for tag in sublist]))}\n",
    "\n",
    "train_dataset = create_dataset(train_tokens, train_tags)\n",
    "dev_dataset = create_dataset(dev_tokens, dev_tags)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained('bert-base-multilingual-cased', num_labels=len(tag2id))\n",
    "model.to(DEVICE)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 518/518 [01:50<00:00,  4.68it/s, loss=0.00189]\n",
      "Epoch 2: 100%|██████████| 518/518 [01:48<00:00,  4.77it/s, loss=0.000982]\n",
      "Epoch 3: 100%|██████████| 518/518 [01:48<00:00,  4.76it/s, loss=0.000557]\n",
      "Epoch 4: 100%|██████████| 518/518 [01:49<00:00,  4.75it/s, loss=0.000336]\n",
      "Epoch 5: 100%|██████████| 518/518 [01:50<00:00,  4.70it/s, loss=0.000221]\n",
      "Evaluating: 100%|██████████| 27/27 [00:01<00:00, 21.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.009484322288596794\n",
      "Average evaluation loss: 0.0001430926355102134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "model.train()\n",
    "train_losses = []\n",
    "train_predictions = []\n",
    "train_true_labels = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    loop = tqdm(train_loader, leave=True)\n",
    "    for batch in loop:\n",
    "        batch = tuple(item.to(DEVICE) for item in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "        \n",
    "        # Remove ignored index (special tokens)\n",
    "        active_accuracy = inputs['attention_mask'].view(-1) == 1\n",
    "        labels = inputs['labels'].view(-1)\n",
    "        predictions = torch.masked_select(predictions.view(-1), active_accuracy)\n",
    "        labels = torch.masked_select(labels, active_accuracy)\n",
    "\n",
    "        train_predictions.extend(predictions.cpu().numpy())\n",
    "        train_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loop.set_description(f\"Epoch {epoch + 1}\")\n",
    "        loop.set_postfix(loss=loss.item())\n",
    "\n",
    "# evaluate\n",
    "model.eval()\n",
    "eval_losses = []\n",
    "eval_predictions = []\n",
    "eval_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(dev_loader, desc=\"Evaluating\"):\n",
    "        batch = tuple(item.to(DEVICE) for item in batch)\n",
    "        inputs = {'input_ids': batch[0], 'attention_mask': batch[1], 'labels': batch[2]}\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        eval_losses.append(loss.item())\n",
    "        predictions = torch.argmax(logits, dim=2)\n",
    "\n",
    "        # Remove ignored index (special tokens)\n",
    "        active_accuracy = inputs['attention_mask'].view(-1) == 1\n",
    "        labels = inputs['labels'].view(-1)\n",
    "        predictions = torch.masked_select(predictions.view(-1), active_accuracy)\n",
    "        labels = torch.masked_select(labels, active_accuracy)\n",
    "\n",
    "        eval_predictions.extend(predictions.cpu().numpy())\n",
    "        eval_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "print(f\"Average training loss: {np.mean(train_losses)}\")\n",
    "print(f\"Average evaluation loss: {np.mean(eval_losses)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.7692028544731209\n",
      "Training Precision: 0.5925232016461537\n",
      "Training Recall: 0.7692028544731209\n",
      "Training F1-Score: 0.669401214729863\n",
      "===============================================\n",
      "Evaluation Accuracy: 0.771695132787047\n",
      "Evaluation Precision: 0.5955133779672183\n",
      "Evaluation Recall: 0.771695132787047\n",
      "Evaluation F1-Score: 0.6722526544738184\n"
     ]
    }
   ],
   "source": [
    "train_acc = accuracy_score(train_true_labels, train_predictions)\n",
    "train_precision, train_recall, train_f1, _ = precision_recall_fscore_support(train_true_labels, train_predictions, average='weighted')\n",
    "\n",
    "print(f\"Training Accuracy: {train_acc}\")\n",
    "print(f\"Training Precision: {train_precision}\")\n",
    "print(f\"Training Recall: {train_recall}\")\n",
    "print(f\"Training F1-Score: {train_f1}\")\n",
    "\n",
    "print(\"===============================================\")\n",
    "\n",
    "eval_acc = accuracy_score(eval_true_labels, eval_predictions)\n",
    "eval_precision, eval_recall, eval_f1, _ = precision_recall_fscore_support(eval_true_labels, eval_predictions, average='weighted')\n",
    "\n",
    "print(f\"Evaluation Accuracy: {eval_acc}\")\n",
    "print(f\"Evaluation Precision: {eval_precision}\")\n",
    "print(f\"Evaluation Recall: {eval_recall}\")\n",
    "print(f\"Evaluation F1-Score: {eval_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
